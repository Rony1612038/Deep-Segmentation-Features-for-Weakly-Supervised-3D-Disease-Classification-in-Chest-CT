{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from dltk.networks.regression_classification.resnet import resnet_3d\n",
    "from dltk.networks.segmentation.unet import residual_unet_3d\n",
    "from dltk.core.activations import leaky_relu\n",
    "from dltk.io.abstract_reader import Reader\n",
    "import json\n",
    "\n",
    "import SimpleITK as sitk\n",
    "from dltk.io.augmentation import flip\n",
    "from dltk.io.preprocessing import whitening\n",
    "from dltk.io.preprocessing import resize_image_with_crop_or_pad\n",
    "# from dltk.io.augmentation import extract_class_balanced_example_array\n",
    "from patch import extract_class_balanced_example_array\n",
    "\n",
    "from tensorflow.contrib import predictor\n",
    "from dltk.io.augmentation import extract_random_example_array\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReaderX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fn(file_references, mode, params=None):\n",
    "    \"\"\"A custom python read function for interfacing with nii image files.\n",
    "    Args:\n",
    "        file_references (list):\n",
    "        mode (str): One of the tf.estimator.ModeKeys strings: TRAIN, EVAL or\n",
    "            PREDICT.\n",
    "        params (dict, optional): A dictionary to parametrise read_fn outputs\n",
    "            (e.g. reader_params = {'n_patches': 10, 'patch_size':\n",
    "            [64, 64, 64], 'extract_patches': True}, etc.).\n",
    "    Yields:\n",
    "        dict: A dictionary of reader outputs for dltk.io.abstract_reader.\n",
    "    \"\"\"\n",
    "\n",
    "    def _augment(img):\n",
    "        return flip(img, axis=2)\n",
    "\n",
    "    for f in file_references:\n",
    "        \n",
    "        # Column 1: Image\n",
    "        img_fn     = str(f[0])\n",
    "        subject_id = img_fn.split('/')[-1].split('.')[0]\n",
    "        img        = sitk.ReadImage(img_fn, sitk.sitkFloat32)       # Loading NIFTI Images via SimpleITK\n",
    "        img        = sitk.GetArrayFromImage(img)\n",
    "        img        = whitening(img)                                 # Whitening Transformation (Variance=1)\n",
    "        \n",
    "        # Column 2: Binary Mask\n",
    "        binary_mask_fn  = str(f[1])\n",
    "        binary_mask     = sitk.GetArrayFromImage(sitk.ReadImage(binary_mask_fn)).astype(np.int32)\n",
    "        \n",
    "        # Column 3: Probabilities Mask\n",
    "        probs_mask_fn   = str(f[2])\n",
    "        probs_mask      = sitk.GetArrayFromImage(sitk.ReadImage(probs_mask_fn, sitk.sitkFloat32))\n",
    "        probs_mask      = np.flip(probs_mask,axis=1)                                                 # ++: Future Optimization: Preprocess in Prior (Flip)\n",
    "        \n",
    "        # Column 4: Feature Maps                                                                     # ++: Future Optimization: Preprocess in Prior (5D->4D)\n",
    "        feature_maps_fn = str(f[3])\n",
    "        # Loading 5D NIFTI Images as 4D Image\n",
    "        file_reader = sitk.ImageFileReader()\n",
    "        file_reader = sitk.ImageFileReader()\n",
    "        file_reader.SetFileName(feature_maps_fn)\n",
    "        file_reader.ReadImageInformation()\n",
    "        feature_maps_size = list(file_reader.GetSize())\n",
    "        file_reader.SetExtractSize([0 if v == 1 else v for v in feature_maps_size])\n",
    "        feature_maps = file_reader.Execute()\n",
    "        feature_maps = sitk.Compose( [sitk.Extract(feature_maps, feature_maps.GetSize()[:3]+(0,), [0,0,0, i]) for i in range(feature_maps_size[-1])] )\n",
    "        feature_maps = sitk.GetArrayFromImage(feature_maps)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # Padding Images\n",
    "        patch_size = params['patch_size']\n",
    "        img_shape  = img.shape\n",
    "        \n",
    "        # Padding Images --Z Dimension\n",
    "        if (patch_size[0] >=img_shape[0]):\n",
    "             zdim = patch_size[0]+10\n",
    "        else:\n",
    "             zdim = img_shape[0]\n",
    "        # Padding Images --X Dimension\n",
    "        if (patch_size[1] >=img_shape[1]):\n",
    "             xdim = patch_size[1]+10\n",
    "        else:\n",
    "             xdim = img_shape[1]\n",
    "        # Padding Images --Y Dimension\n",
    "        if (patch_size[2] >=img_shape[2]):\n",
    "             ydim = patch_size[2]+10\n",
    "        else:\n",
    "             ydim = img_shape[2]\n",
    "        \n",
    "        img            = resize_image_with_crop_or_pad(img,          [zdim,xdim,ydim],                         mode='symmetric')\n",
    "        binary_mask    = resize_image_with_crop_or_pad(binary_mask,  [zdim,xdim,ydim],                         mode='symmetric')\n",
    "        probs_mask     = resize_image_with_crop_or_pad(probs_mask,   [zdim,xdim,ydim],                         mode='symmetric')\n",
    "        feature_maps    = resize_image_with_crop_or_pad(feature_maps,  [zdim,xdim,ydim,feature_maps.shape[3]], mode='symmetric')\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # Column 5: Label\n",
    "        lbl         = np.int(f[4])\n",
    "        lbl         = np.expand_dims(lbl, axis=-1).astype(np.int32)\n",
    "        \n",
    "        # Expand to 4D\n",
    "        img         = np.expand_dims(img, axis=3)\n",
    "        probs_mask  = np.expand_dims(probs_mask, axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        # Probabilities + Feature Map Concatenation\n",
    "        connect = 'max'\n",
    "        if connect=='probs_mask':\n",
    "            img = np.concatenate((img,probs_mask),axis=3)\n",
    "        elif connect=='feature_maps':    \n",
    "            img = np.concatenate((img,feature_maps),axis=3)\n",
    "        elif connect=='max':\n",
    "            img = np.concatenate((img,probs_mask,feature_maps),axis=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # TensorFlow Mode-Based Execution\n",
    "        # Prediction Mode\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            yield {'features': {'x': img},\n",
    "                   'labels':   {'y': lbl.astype(np.float32)}, \n",
    "                   'img_id':         subject_id}\n",
    "        \n",
    "        # Training Mode\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            pass\n",
    "        \n",
    "        # Augmentation Flag    \n",
    "        if params['augmentation']:\n",
    "            img = _augment(img)\n",
    "\n",
    "        # Return Training Examples\n",
    "        if params['extract_patches']:\n",
    "            img,binary_mask = extract_class_balanced_example_array(\n",
    "                img,binary_mask,\n",
    "                example_size  = params['patch_size'],\n",
    "                n_examples    = params['n_patches'],\n",
    "                classes       = 4, class_weights=[0,0,1,1]   # Label 3,4 => Right,Left Lungs (XCAT)\n",
    "                )\n",
    "\n",
    "            for e in range(params['n_patches']):\n",
    "                yield {'features': {'x': img[e].astype(np.float32)},\n",
    "                       'labels':   {'y': lbl.astype(np.float32)},\n",
    "                       'img_id':         subject_id}\n",
    "\n",
    "        # Return Full Images\n",
    "        else:\n",
    "            yield {'features': {'x': img},\n",
    "                   'labels':   {'y': lbl.astype(np.float32)},\n",
    "                   'img_id':         subject_id}\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x):\n",
    "    return leaky_relu(x, 0.1)\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "\n",
    "    # Segmentation Model Definition (3D Residual U-Net)\n",
    "    segnet_output_ops = residual_unet_3d(\n",
    "        inputs              = features['x'],                               # Input: Patches (dimensions=128cube; defined by 'patch_size')\n",
    "        num_classes         = 2,\n",
    "        num_res_units       = 3,\n",
    "        filters             = [16, 64, 128, 256, 512],\n",
    "        strides             = ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (1, 1, 1)),\n",
    "        mode                = tf.estimator.ModeKeys.EVAL,\n",
    "        activation          = lrelu,\n",
    "        kernel_initializer  = tf.initializers.variance_scaling(distribution='uniform'),\n",
    "        bias_initializer    = tf.zeros_initializer(),\n",
    "        kernel_regularizer  = None,\n",
    "        bias_regularizer    = None)\n",
    "\n",
    "\n",
    "\n",
    "    # Classification Model Definition (3D ResNet)\n",
    "    clfnet_output_ops = resnet_3d(\n",
    "        inputs              = features['x'],                # Input: Output of 3D Residual U-Net (dimensions=128cube)\n",
    "        num_res_units       = 2,\n",
    "        num_classes         = NUM_CLASSES,\n",
    "        filters             = (16, 32, 64, 128, 256),\n",
    "        strides             = ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n",
    "        kernel_initializer  = tf.initializers.variance_scaling(distribution='uniform'),\n",
    "        bias_initializer    = tf.zeros_initializer(),\n",
    "        mode                = mode,\n",
    "        kernel_regularizer  = tf.contrib.layers.l2_regularizer(1e-3))\n",
    "\n",
    "    # Prediction Mode\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode            = mode,\n",
    "            predictions     = clfnet_output_ops,\n",
    "            export_outputs  = {'out': tf.estimator.export.PredictOutput(clfnet_output_ops)})\n",
    "\n",
    "    # Loss Function\n",
    "    one_hot_labels = tf.reshape(tf.one_hot(labels['y'], depth=NUM_CLASSES), [-1, NUM_CLASSES])\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels      = one_hot_labels,\n",
    "        logits             = clfnet_output_ops['logits'])\n",
    "\n",
    "    # Optimizer\n",
    "    global_step = tf.train.get_global_step()\n",
    "    if params[\"opt\"] == 'adam':\n",
    "        optimiser = tf.train.AdamOptimizer(\n",
    "            learning_rate=params[\"learning_rate\"], epsilon=1e-5)\n",
    "    elif params[\"opt\"] == 'momentum':\n",
    "        optimiser = tf.train.MomentumOptimizer(\n",
    "            learning_rate=params[\"learning_rate\"], momentum=0.9)\n",
    "    elif params[\"opt\"] == 'rmsprop':\n",
    "        optimiser = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=params[\"learning_rate\"], momentum=0.9)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimiser.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Custom Image Summaries (TensorBoard)\n",
    "    my_image_summaries = {}\n",
    "    my_image_summaries['CT_Patch'] = features['x'][0, 32, :, :, 0]\n",
    "\n",
    "    expected_output_size = [1, 128, 128, 1]  # [B, W, H, C]\n",
    "    [tf.summary.image(name, tf.reshape(image, expected_output_size))\n",
    "     for name, image in my_image_summaries.items()]\n",
    "\n",
    "    # Track RMSE\n",
    "    acc             = tf.metrics.accuracy\n",
    "    prec            = tf.metrics.precision\n",
    "    auc             = tf.metrics.auc\n",
    "    eval_metric_ops = {\"accuracy\":  acc(labels['y'],  clfnet_output_ops['y_']),\n",
    "                       \"precision\": prec(labels['y'], clfnet_output_ops['y_']),\n",
    "                       \"auc\":       prec(labels['y'], clfnet_output_ops['y_'])}\n",
    "\n",
    "    # Return EstimatorSpec Object\n",
    "    return tf.estimator.EstimatorSpec(mode            = mode,\n",
    "                                      predictions     = clfnet_output_ops,\n",
    "                                      loss            = loss,\n",
    "                                      train_op        = train_op,\n",
    "                                      eval_metric_ops = eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_steps = []\n",
    "count_loss  = []\n",
    "\n",
    "\n",
    "EVAL_EVERY_N_STEPS = 500\n",
    "EVAL_STEPS         = 10\n",
    "\n",
    "PATCH              = 96\n",
    "NUM_CLASSES        = 2\n",
    "NUM_CHANNELS       = 63\n",
    "CONNECT            = 'max'\n",
    "\n",
    "BATCH_SIZE         = 3\n",
    "SHUFFLE_CACHE_SIZE = 64\n",
    "\n",
    "MAX_STEPS          = 150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Arguments\n",
    "with open('config.json') as f:\n",
    "    run_config = json.load(f)\n",
    "    \n",
    "train_filenames = pd.read_csv(\n",
    "    '.\\csvIII\\Lung_CV-Training-Fold-1.csv', dtype=object, keep_default_na=False,\n",
    "    na_values=[]).values\n",
    "\n",
    "val_filenames = pd.read_csv(\n",
    "    '.\\csvIII\\Lung_CV-Validation-Fold-1.csv', dtype=object, keep_default_na=False,\n",
    "    na_values=[]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DLTK Reader Parameters (No. of Patches, Patch Size) \n",
    "reader_params = {'n_patches': 2,\n",
    "                 'patch_size': [PATCH, PATCH, PATCH],   # Target Patch Size\n",
    "                 'extract_patches': True,                # Enable Training Mode Patch Extraction\n",
    "                 'augmentation':True}\n",
    "# Set Patch Dimensions\n",
    "reader_patch_shapes = {'features': {'x': reader_params['patch_size'] + [NUM_CHANNELS]},\n",
    "                       'labels':   {'y': [1]}}\n",
    "# Initiate Data Reader + Patch Extraction\n",
    "reader = Reader(read_fn,\n",
    "              {'features': {'x': tf.float32},\n",
    "               'labels':   {'y': tf.int32}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Input Functions + Queue Initialisation Hooks for Training/Validation Data\n",
    "train_input_fn, train_qinit_hook = reader.get_inputs(\n",
    "    file_references       = train_filenames,\n",
    "    mode                  = tf.estimator.ModeKeys.TRAIN,\n",
    "    example_shapes        = reader_patch_shapes,\n",
    "    batch_size            = BATCH_SIZE,\n",
    "    shuffle_cache_size    = SHUFFLE_CACHE_SIZE,\n",
    "    params                = reader_params)\n",
    "val_input_fn, val_qinit_hook = reader.get_inputs(\n",
    "    file_references       = val_filenames,\n",
    "    mode                  = tf.estimator.ModeKeys.EVAL,\n",
    "    example_shapes        = reader_patch_shapes,\n",
    "    batch_size            = BATCH_SIZE,\n",
    "    shuffle_cache_size    = SHUFFLE_CACHE_SIZE,\n",
    "    params                = reader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_num_worker_replicas': 1, '_device_fn': None, '_experimental_distribute': None, '_task_id': 0, '_eval_distribute': None, '_evaluation_master': '', '_save_checkpoints_secs': 600, '_task_type': 'worker', '_protocol': None, '_model_dir': './weights/17072019/Fold_1', '_is_chief': True, '_tf_random_seed': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_train_distribute': None, '_keep_checkpoint_every_n_hours': 10000, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002289B119CC0>, '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_master': '', '_service': None, '_save_summary_steps': 100, '_global_id_in_cluster': 0}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Neural Network Estimator\n",
    "nn = tf.estimator.Estimator(\n",
    "    model_fn             = model_fn,\n",
    "    model_dir            = './weights/17072019/Fold_1',\n",
    "    params               = run_config,\n",
    "    config               = tf.estimator.RunConfig())\n",
    "\n",
    "# Hooks for Validation Summaries\n",
    "val_summary_hook = tf.contrib.training.SummaryAtEndHook(\n",
    "    os.path.join('./weights/17072019/Fold_1', 'eval'))\n",
    "step_cnt_hook = tf.train.StepCounterHook(every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "                                         output_dir='./weights/17072019/Fold_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.get_shape of <tf.Tensor 'zeros:0' shape=(3, 96, 96, 96, 63) dtype=float32>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tf.zeros([3, 96, 96, 96, 63], dtype=tf.float32)\n",
    "labels = [1]\n",
    "input.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation Model Definition (3D Residual U-Net)\n",
    "segnet_output_ops = residual_unet_3d(\n",
    "    inputs              = input,                               # Input: Patches (dimensions=128cube; defined by 'patch_size')\n",
    "    num_classes         = 2,\n",
    "    num_res_units       = 3,\n",
    "    filters             = [16, 64, 128, 256, 512],\n",
    "    strides             = ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (1, 1, 1)),\n",
    "    mode                = tf.estimator.ModeKeys.EVAL,\n",
    "    activation          = lrelu,\n",
    "    kernel_initializer  = tf.initializers.variance_scaling(distribution='uniform'),\n",
    "    bias_initializer    = tf.zeros_initializer(),\n",
    "    kernel_regularizer  = None,\n",
    "    bias_regularizer    = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segnet_output_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anindo-saha\\Anaconda3\\envs\\tensorflow-RAI\\lib\\site-packages\\dltk\\networks\\regression_classification\\resnet.py:77: conv3d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv3d instead.\n",
      "WARNING:tensorflow:From C:\\Users\\anindo-saha\\Anaconda3\\envs\\tensorflow-RAI\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Init conv tensor shape (3, 96, 96, 96, 16)\n",
      "WARNING:tensorflow:From C:\\Users\\anindo-saha\\Anaconda3\\envs\\tensorflow-RAI\\lib\\site-packages\\dltk\\core\\residual_unit.py:71: max_pooling3d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling3d instead.\n",
      "WARNING:tensorflow:From C:\\Users\\anindo-saha\\Anaconda3\\envs\\tensorflow-RAI\\lib\\site-packages\\dltk\\core\\residual_unit.py:80: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "INFO:tensorflow:Encoder at res_scale 1 tensor shape: (3, 48, 48, 48, 32)\n",
      "INFO:tensorflow:Encoder at res_scale 2 tensor shape: (3, 24, 24, 24, 64)\n",
      "INFO:tensorflow:Encoder at res_scale 3 tensor shape: (3, 12, 12, 12, 128)\n",
      "INFO:tensorflow:Encoder at res_scale 4 tensor shape: (3, 6, 6, 6, 256)\n",
      "INFO:tensorflow:Global pool shape (3, 256)\n",
      "WARNING:tensorflow:From C:\\Users\\anindo-saha\\Anaconda3\\envs\\tensorflow-RAI\\lib\\site-packages\\dltk\\networks\\regression_classification\\resnet.py:132: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "INFO:tensorflow:Output tensor shape (3, 2)\n"
     ]
    }
   ],
   "source": [
    "# Classification Model Definition (3D ResNet)\n",
    "clfnet_output_ops = resnet_3d(\n",
    "    inputs              = input,                # Input: Output of 3D Residual U-Net (dimensions=128cube)\n",
    "    num_res_units       = 2,\n",
    "    num_classes         = NUM_CLASSES,\n",
    "    filters             = (16, 32, 64, 128, 256),\n",
    "    strides             = ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n",
    "    kernel_initializer  = tf.initializers.variance_scaling(distribution='uniform'),\n",
    "    bias_initializer    = tf.zeros_initializer(),\n",
    "    mode                = tf.estimator.ModeKeys.TRAIN,\n",
    "    kernel_regularizer  = tf.contrib.layers.l2_regularizer(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xclfnet_output_ops = resnet_3d(\n",
    "    inputs              = tf.concat(values=[input,segnet_output_ops['logits']], axis=4),                # Input: Output of 3D Residual U-Net (dimensions=128cube)\n",
    "    num_res_units       = 2,\n",
    "    num_classes         = NUM_CLASSES,\n",
    "    filters             = (16, 32, 64, 128, 256),\n",
    "    strides             = ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n",
    "    kernel_initializer  = tf.initializers.variance_scaling(distribution='uniform'),\n",
    "    bias_initializer    = tf.zeros_initializer(),\n",
    "    mode                = tf.estimator.ModeKeys.TRAIN,\n",
    "    kernel_regularizer  = tf.contrib.layers.l2_regularizer(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "one_hot_labels = tf.reshape(tf.one_hot(labels, depth=NUM_CLASSES), [-1, NUM_CLASSES])\n",
    "loss = tf.losses.softmax_cross_entropy(\n",
    "    onehot_labels      = one_hot_labels,\n",
    "    logits             = xclfnet_output_ops['logits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch =   tf.zeros([1, 128, 128, 128, 1], dtype=tf.float32)\n",
    "feature = tf.ones([1, 128, 128, 128, 2], dtype=tf.float32)\n",
    "patch.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.concat(values=[patch,feature], axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.array([0,1,6])\n",
    "m.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "READER_PARAMS = {'extract_patches':          False,\n",
    "                 'augmentation':             False,\n",
    "                 'n_patches':                    1,\n",
    "                 'patch_size':      [128, 128, 128]}\n",
    "\n",
    "model_path = 'idealU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list              =  []     # Patient ID\n",
    "probability_list     =  []     # Probabilities\n",
    "label_list           =  []     # Labels\n",
    "class_1_list         =  []     # Class 1\n",
    "class_2_list         =  []     # Class 2\n",
    "\n",
    "# Read CSV with Validation/Test Set\n",
    "file_names = pd.read_csv(\n",
    "    '.\\csv\\Lung_CV-Training-Fold-1.csv',\n",
    "    dtype=object,\n",
    "    keep_default_na=False,\n",
    "    na_values=[]).values\n",
    "\n",
    "\n",
    "# Load Trained Model\n",
    "export_dir = \\\n",
    "    [os.path.join(model_path, o) for o in sorted(os.listdir(model_path))\n",
    "     if os.path.isdir(os.path.join(model_path, o)) and o.isdigit()][-1]\n",
    "print('Loading from {}'.format(export_dir))\n",
    "\n",
    "my_predictor = predictor.from_saved_model(export_dir)\n",
    "# Iterate through Files, Predict on the Full Volumes, Compute Dice\n",
    "accuracy = []\n",
    "for output in read_fn(file_references = file_names,\n",
    "                      mode            = tf.estimator.ModeKeys.PREDICT,\n",
    "                      params          = READER_PARAMS):\n",
    "    \n",
    "    t0 = time.time()  # Timing Function\n",
    "    # Parse Data Reader Output\n",
    "    img      = output['features']['x']\n",
    "    lbl      = output['labels']['y']\n",
    "    test_id  = output['img_id']\n",
    "    # Decompose Volumes into Patches        \n",
    "    num_crop_predictions = 8\n",
    "    crop_batch = extract_random_example_array(\n",
    "        image_list     = img,\n",
    "        example_size   = [128, 128, 128],\n",
    "        n_examples     = num_crop_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictor._fetch_tensors['y_prob']\n",
    "# my_predictor._feed_tensors['x']: crop_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictor._fetch_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = my_predictor._fetch_tensors['y_prob']\n",
    "y_probsx = [(my_predictor._fetch_tensors['y_prob'])]\n",
    "y_probsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# residual_3DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from dltk.core.residual_unit import vanilla_residual_unit_3d\n",
    "from dltk.core.upsample import linear_upsample_3d\n",
    "from dltk.core.activations import leaky_relu\n",
    "\n",
    "\n",
    "\n",
    "# Auxiliary Functions\n",
    "def upsample_and_concat(inputs, inputs2, strides=(2, 2, 2)):\n",
    "    \"\"\"Upsampling and concatenation layer according to [1].\n",
    "\n",
    "    [1] O. Ronneberger et al. U-Net: Convolutional Networks for Biomedical Image\n",
    "        Segmentation. MICCAI 2015.\n",
    "\n",
    "    Args:\n",
    "        inputs (TYPE): Input features to be upsampled.\n",
    "        inputs2 (TYPE): Higher resolution features from the encoder to\n",
    "            concatenate.\n",
    "        strides (tuple, optional): Upsampling factor for a strided transpose\n",
    "            convolution.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Upsampled feature tensor\n",
    "    \"\"\"\n",
    "    assert len(inputs.get_shape().as_list()) == 5, \\\n",
    "        'inputs are required to have a rank of 5.'\n",
    "    assert len(inputs.get_shape().as_list()) == len(inputs2.get_shape().as_list()), \\\n",
    "        'Ranks of input and input2 differ'\n",
    "\n",
    "    # Upsample inputs\n",
    "    inputs = linear_upsample_3d(inputs, strides)\n",
    "\n",
    "    return tf.concat(axis=-1, values=[inputs2, inputs])\n",
    "\n",
    "\n",
    "# Target Model\n",
    "def residual_3DPNet (inputs,\n",
    "                     num_classes,\n",
    "                     mode                        = tf.estimator.ModeKeys.EVAL,\n",
    "                     \n",
    "                     seg__num_res_units          = 1,\n",
    "                     seg__filters                = (16, 32, 64, 128),\n",
    "                     seg__strides                = ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n",
    "                     seg__use_bias               = False,\n",
    "                     seg__activation             = leaky_relu,\n",
    "                     seg__kernel_initializer     = tf.initializers.variance_scaling(distribution='uniform'),\n",
    "                     seg__bias_initializer       = tf.zeros_initializer(),\n",
    "                     seg__kernel_regularizer     = None,\n",
    "                     seg__bias_regularizer       = None,\n",
    "                     seg__bottleneck             = False,\n",
    "                     \n",
    "                     clf__num_res_units          = 1,\n",
    "                     clf__filters                = (16, 32, 64, 128),\n",
    "                     clf__strides                = ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2)), \n",
    "                     clf__use_bias               = False,\n",
    "                     clf__activation             = tf.nn.relu6,\n",
    "                     clf__kernel_initializer     = tf.initializers.variance_scaling(distribution='uniform'),\n",
    "                     clf__bias_initializer       = tf.zeros_initializer(),\n",
    "                     clf__kernel_regularizer     = None, \n",
    "                     clf__bias_regularizer       = None):\n",
    "\n",
    "    \n",
    "    # RESIDUAL 3D U-NET (residual_3D_unet) // SEGMENTATION {\n",
    "    \"\"\"\n",
    "    Image segmentation network based on a flexible UNET architecture [1]\n",
    "    using residual units [2] as feature extractors. Downsampling and\n",
    "    upsampling of features is done via strided convolutions and transpose\n",
    "    convolutions, respectively. On each resolution scale s are\n",
    "    num_residual_units with filter size = filters[s]. strides[s] determine\n",
    "    the downsampling factor at each resolution scale.\n",
    "     \n",
    "    [1] O. Ronneberger et al. U-Net: Convolutional Networks for Biomedical Image\n",
    "        Segmentation. MICCAI 2015.\n",
    "    [2] K. He et al. Identity Mappings in Deep Residual Networks. ECCV 2016.\n",
    "    \n",
    "    Args:\n",
    "        inputs (tf.Tensor): Input feature tensor to the network (rank 5\n",
    "            required).\n",
    "        num_classes (int): Number of output classes.\n",
    "        num_res_units (int, optional): Number of residual units at each\n",
    "            resolution scale.\n",
    "        filters (tuple, optional): Number of filters for all residual units at\n",
    "            each resolution scale.\n",
    "        strides (tuple, optional): Stride of the first unit on a resolution\n",
    "            scale.\n",
    "        mode (TYPE, optional): One of the tf.estimator.ModeKeys strings: TRAIN,\n",
    "            EVAL or PREDICT\n",
    "        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n",
    "        activation (optional): A function to use as activation function.\n",
    "        kernel_initializer (TYPE, optional): An initializer for the convolution\n",
    "            kernel.\n",
    "        bias_initializer (TYPE, optional): An initializer for the bias vector.\n",
    "            If None, no bias will be applied.\n",
    "        kernel_regularizer (None, optional): Optional regularizer for the\n",
    "            convolution kernel.\n",
    "        bias_regularizer (None, optional): Optional regularizer for the bias\n",
    "            vector.\n",
    "    \n",
    "    Returns:\n",
    "        dict: dictionary of output tensors\n",
    "    \n",
    "    }\"\"\"\n",
    "    # Input:  CT Patch Features \n",
    "    # Output: Segmentation Feature Maps\n",
    "    \n",
    "    \n",
    "    # INITIALIZATION\n",
    "    assert len(seg__strides) == len(seg__filters)\n",
    "    assert len(inputs.get_shape().as_list()) == 5, \\\n",
    "        'Inputs are required to have a rank of 5.'\n",
    "    \n",
    "    seg__conv_params = {'padding':              'same',\n",
    "                        'use_bias':             seg__use_bias,\n",
    "                        'kernel_initializer':   seg__kernel_initializer,\n",
    "                        'bias_initializer':     seg__bias_initializer,\n",
    "                        'kernel_regularizer':   seg__kernel_regularizer,\n",
    "                        'bias_regularizer':     seg__bias_regularizer}\n",
    "    x = inputs\n",
    "    \n",
    "    # Initial Convolution with (seg__filters[0])\n",
    "    x = tf.layers.conv3d(inputs       = x,\n",
    "                         filters      = seg__filters[0],\n",
    "                         kernel_size  = (3, 3, 3),\n",
    "                         strides      = seg__strides[0],\n",
    "                         **seg__conv_params)\n",
    "    tf.logging.info('Segmentation Network: Initial Conv3D Tensor Shape: {}'.format(x.get_shape()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # FEATURE EXTRACTOR // ENCODER:\n",
    "    # Residual Blocks with (seg__num_res_units) at Different Resolution Scales (res_scales)\n",
    "    res_scales = [x]\n",
    "    saved_strides = []\n",
    "    for res_scale in range(1, len(seg__filters)):\n",
    "     \n",
    "        # Features are Downsampled via Strided Convolutions ('seg__strides')\n",
    "        with tf.variable_scope('enc_unit_{}_0'.format(res_scale)):\n",
    "            x = vanilla_residual_unit_3d(\n",
    "                inputs            = x,\n",
    "                out_filters       = seg__filters[res_scale],\n",
    "                strides           = seg__strides[res_scale],\n",
    "                activation        = seg__activation,\n",
    "                mode              = mode)\n",
    "        saved_strides.append(seg__strides[res_scale])\n",
    "        \n",
    "        for i in range(1, seg__num_res_units):\n",
    "            with tf.variable_scope('enc_unit_{}_{}'.format(res_scale, i)):\n",
    "                x = vanilla_residual_unit_3d(\n",
    "                    inputs        = x,\n",
    "                    out_filters   = seg__filters[res_scale],\n",
    "                    strides       = (1, 1, 1),\n",
    "                    activation    = seg__activation,\n",
    "                    mode          = mode)\n",
    "        res_scales.append(x)\n",
    "        tf.logging.info('Segmentation Network: Encoder at \"res_scale\" {} Tensor Shape: {}'.format(\n",
    "            res_scale, x.get_shape()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # RESTORE SPATIAL DIMENSION // DECODER:\n",
    "    # Upsample and Concatenate Layers and Reconstruct Predictions to Higher Resolution Scales\n",
    "    for res_scale in range(len(seg__filters) - 2, -1, -1):\n",
    "        \n",
    "        with tf.variable_scope('up_concat_{}'.format(res_scale)):\n",
    "            x = upsample_and_concat(\n",
    "                inputs           = x,\n",
    "                inputs2          = res_scales[res_scale],\n",
    "                strides          = saved_strides[res_scale])\n",
    "        \n",
    "        for i in range(0, seg__num_res_units):\n",
    "            with tf.variable_scope('dec_unit_{}_{}'.format(res_scale, i)):\n",
    "                x = vanilla_residual_unit_3d(\n",
    "                    inputs       = x,\n",
    "                    out_filters  = seg__filters[res_scale],\n",
    "                    strides      = (1, 1, 1),\n",
    "                    mode         = mode)\n",
    "        tf.logging.info('Segmentation Network: Decoder at \"res_scale\" {} Tensor Shape: {}'.format(\n",
    "            res_scale, x.get_shape()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # BOTTLENECK CONVOLUTION\n",
    "    if seg__bottleneck:\n",
    "     with tf.variable_scope('last'):\n",
    "        \n",
    "         x = tf.layers.conv3d(inputs         = x,\n",
    "                              filters        = num_classes,\n",
    "                              kernel_size    = (1, 1, 1),\n",
    "                              strides        = (1, 1, 1),\n",
    "                              **seg__conv_params)\n",
    "     tf.logging.info('Segmentation Network: Bottleneck Output Tensor Shape: {}'.format(x.get_shape()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # OUTPUT\n",
    "    residual3Dunet_output = x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3D RESNET (resnet_3d) // CLASSIFICATION {\n",
    "    \"\"\"\n",
    "    Regression/classification network based on a flexible resnet\n",
    "    architecture [1] using residual units proposed in [2]. The downsampling\n",
    "    of features is done via strided convolutions. On each resolution scale s\n",
    "    are num_convolutions with filter size = filters[s]. strides[s]\n",
    "    determine the downsampling factor at each resolution scale.\n",
    "    \n",
    "    [1] K. He et al. Deep residual learning for image recognition. CVPR 2016.\n",
    "    [2] K. He et al. Identity Mappings in Deep Residual Networks. ECCV 2016.\n",
    "    \n",
    "    Args:\n",
    "        inputs (tf.Tensor): Input feature tensor to the network (rank 5\n",
    "            required).\n",
    "        num_classes (int): Number of output channels or classes.\n",
    "        num_res_units (int, optional): Number of residual units per resolution\n",
    "            scale.\n",
    "        filters (tuple, optional): Number of filters for all residual units at\n",
    "            each resolution scale.\n",
    "        strides (tuple, optional): Stride of the first unit on a resolution\n",
    "            scale.\n",
    "        mode (TYPE, optional): One of the tf.estimator.ModeKeys strings: TRAIN,\n",
    "            EVAL or PREDICT\n",
    "        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n",
    "        activation (optional): A function to use as activation function.\n",
    "        kernel_initializer (TYPE, optional): An initializer for the convolution\n",
    "            kernel.\n",
    "        bias_initializer (TYPE, optional): An initializer for the bias vector.\n",
    "            If None, no bias will be applied.\n",
    "        kernel_regularizer (None, optional): Optional regularizer for the\n",
    "            convolution kernel.\n",
    "        bias_regularizer (None, optional): Optional regularizer for the bias\n",
    "            vector.\n",
    "    \n",
    "    Returns:\n",
    "        dict: dictionary of output tensors\n",
    "    \n",
    "    }\"\"\"\n",
    "    # Input:  CT Patch Features (concat) Segmentation Feature Maps\n",
    "    # Output: Classification Scores (logits, y_prob, y_)\n",
    "\n",
    "\n",
    "    # INITIALIZATION\n",
    "    resnet3D_output = {}\n",
    "    assert len(clf__strides) == len(clf__filters)\n",
    "    assert len(residual3Dunet_output.get_shape().as_list()) == 5, \\\n",
    "        'Inputs are required to have a rank of 5.'\n",
    "    \n",
    "    relu_op = tf.nn.relu6\n",
    "    \n",
    "    clf__conv_params = {'padding':              'same',\n",
    "                        'use_bias':             clf__use_bias,\n",
    "                        'kernel_initializer':   clf__kernel_initializer,\n",
    "                        'bias_initializer':     clf__bias_initializer,\n",
    "                        'kernel_regularizer':   clf__kernel_regularizer,\n",
    "                        'bias_regularizer':     clf__bias_regularizer}\n",
    "    \n",
    "    # Concatenated CT and Segmentation Features --> Input Feed \n",
    "    x = tf.concat(values=[inputs, residual3Dunet_output], axis=4)\n",
    "    \n",
    "    # Initial Convolution with (clf__filters[0])\n",
    "    k = [s * 2 if s > 1 else 3 for s in clf__strides[0]]\n",
    "    x = tf.layers.conv3d(x, clf__filters[0], k, clf__strides[0], **clf__conv_params)\n",
    "    tf.logging.info('Classification Network: Initial Conv3D Tensor Shape: {}'.format(x.get_shape()))\n",
    "    \n",
    "    \n",
    "       \n",
    "    \n",
    "    # FEATURE EXTRACTOR // ENCODER:\n",
    "    # Residual Blocks with (clf__num_res_units) at Different Resolution Scales (res_scales)\n",
    "    res_scales = [x]\n",
    "    saved_strides = []\n",
    "    for res_scale in range(1, len(clf__filters)):\n",
    "        \n",
    "        # Features are Downsampled via Strided Convolutions ('clf__strides')\n",
    "        with tf.variable_scope('unit_{}_0'.format(res_scale)):\n",
    "            x = vanilla_residual_unit_3d(\n",
    "                inputs                = x,\n",
    "                out_filters           = clf__filters[res_scale],\n",
    "                strides               = clf__strides[res_scale],\n",
    "                activation            = clf__activation,\n",
    "                mode                  = mode)\n",
    "        saved_strides.append(clf__strides[res_scale])\n",
    "        \n",
    "        for i in range(1, clf__num_res_units):\n",
    "            with tf.variable_scope('unit_{}_{}'.format(res_scale, i)):\n",
    "                x = vanilla_residual_unit_3d(\n",
    "                    inputs            = x,\n",
    "                    out_filters       = clf__filters[res_scale],\n",
    "                    strides           = (1, 1, 1),\n",
    "                    activation        = clf__activation,\n",
    "                    mode              = mode)\n",
    "        res_scales.append(x)\n",
    "        tf.logging.info('Classification Network: Encoder at \"res_scale\" {} Tensor Shape: {}'.format(\n",
    "            res_scale, x.get_shape()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # GLOBAL POOLING + FINAL LAYER\n",
    "    with tf.variable_scope('pool'):\n",
    "        x = tf.layers.batch_normalization(\n",
    "            x, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        x = relu_op(x)\n",
    "        \n",
    "        axis = tuple(range(len(x.get_shape().as_list())))[1:-1]\n",
    "        x = tf.reduce_mean(x, axis=axis, name='global_avg_pool')\n",
    "        \n",
    "        tf.logging.info('Classification Network: Global Pooling Tensor Shape: {}'.format(x.get_shape()))\n",
    "    \n",
    "    with tf.variable_scope('last'):\n",
    "        x = tf.layers.dense(inputs                = x,\n",
    "                            units                 = num_classes,\n",
    "                            activation            = None,\n",
    "                            use_bias              = clf__conv_params['use_bias'],\n",
    "                            kernel_initializer    = clf__conv_params['kernel_initializer'],\n",
    "                            bias_initializer      = clf__conv_params['bias_initializer'],\n",
    "                            kernel_regularizer    = clf__conv_params['kernel_regularizer'],\n",
    "                            bias_regularizer      = clf__conv_params['bias_regularizer'],\n",
    "                            name                  = 'hidden_units')\n",
    "        \n",
    "        tf.logging.info('Classification Network: Output Tensor Shape: {}'.format(x.get_shape()))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # OUTPUT\n",
    "    resnet3D_output['logits'] = x\n",
    "    \n",
    "    with tf.variable_scope('pred'):\n",
    "        \n",
    "        y_prob = tf.nn.softmax(x)\n",
    "        resnet3D_output['y_prob'] = y_prob\n",
    "        \n",
    "        y_ = tf.argmax(x, axis=-1) \\\n",
    "            if num_classes > 1 \\\n",
    "            else tf.cast(tf.greater_equal(x[..., 0], 0.5), tf.int32)\n",
    "        resnet3D_output['y_'] = y_\n",
    "    \n",
    "    return resnet3D_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition (residual_3DPNet)\n",
    "model_output_ops = residual_3DPNet(\n",
    "    # Input: Patches (dimensions=128cube,channel=1; defined by 'patch_size')\n",
    "    inputs                   = input,                               \n",
    "    num_classes              = 2,\n",
    "    mode                     = tf.estimator.ModeKeys.TRAIN,\n",
    "    \n",
    "    seg__num_res_units       = 1,\n",
    "    seg__filters             = [8, 16],\n",
    "    seg__strides             = ((1, 1, 1), (1, 1, 1)),\n",
    "    seg__activation          = lrelu,\n",
    "    seg__kernel_initializer  = tf.initializers.variance_scaling(distribution='uniform'),\n",
    "    seg__bias_initializer    = tf.zeros_initializer(),\n",
    "    seg__kernel_regularizer  = None,\n",
    "    seg__bottleneck          = False,\n",
    "            \n",
    "    clf__num_res_units       = 2,\n",
    "    clf__filters             = (16, 32, 64, 128, 256),\n",
    "    clf__strides             = ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n",
    "    clf__kernel_initializer  = tf.initializers.variance_scaling(distribution='uniform'),\n",
    "    clf__bias_initializer    = tf.zeros_initializer(),\n",
    "    clf__kernel_regularizer  = tf.contrib.layers.l2_regularizer(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.concat(values=[input, input], axis=4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.zeros([1, 128, 128, 128, 4], dtype=tf.float32)\n",
    "F     = tf.concat(values=[input, input], axis=4)\n",
    "shape = F.get_shape().as_list()\n",
    "ydim        = shape[2]\n",
    "xdim        = shape[3]\n",
    "featuremaps = shape[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = tf.slice(F,(0,0,0,0,0),(1,1,-1,-1,-1)) \n",
    "F = tf.reshape(F,(ydim,xdim,featuremaps))\n",
    "F.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydim += 4\n",
    "xdim += 4\n",
    "F = tf.image.resize_image_with_crop_or_pad(F,ydim,xdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = tf.reshape(F,(ydim,xdim,4,2)) \n",
    "F = tf.transpose(F,(2,0,3,1))\n",
    "F = tf.reshape(F,(1,4*ydim,2*xdim,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.image('Segmentation Feature Maps', F, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-RAI)",
   "language": "python",
   "name": "tensorflow-rai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
